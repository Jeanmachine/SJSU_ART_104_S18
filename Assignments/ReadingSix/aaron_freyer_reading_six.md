Invisible Images


	This essay by Trevor Paglen discusses the meaning of digital images—how most digital images are created for machines and viewed only by machines. The author examples of images never seen by humans "machine-to-machine seeing apparatuses." I think the thesis of this piece is summed up in the conclusion: “We no longer look at images–images look at us.” Up until a certain point in modern history, images were used as a more accurate depiction of something than a painting. I could commision a portrait photograph of myself, I could take pictures on a family vacation with a disposable camera, or I can snap a picture of the whiteboard in class rather than take notes. In many ways photos become an effective means of documenting a situation, or an artistic expression of the producer behind the camera. 
	
	Prior to reading this article, I had never thought of images as being anything other than what humans see. Examples noted by Paglen, such as license plate scanners, law enforcement cameras, and social media AI backends suggest an eye-opening consideration— how powerful and large is computer intelligence and the network(s) they operate on? The reality we live in today is that, according to Paglen, most pictures that exist are digital. Most picture has no “original” as in film, and most digital pictures are never viewed by human beings. This visual information captured by technology— like the facial expressions of shoppers— is being used mostly for the profit and gain of privately held companies. At first glance this seems like a frightening prospect (or reality). But is it really?
	
I think about this “data” collected, for use by government, insurance companies, and more. This does fear me slightly, however I take a fairly accepting approach. I would like to think that technology works to serve humans and better our lives. I would like to think that my facial expression isn’t being viewed by human beings, but is serving me to deliver me a better shopping experience or relevant content. I believe latent effects of our increasing dependence of AI will begin to surface, and must be worked through one at a time. 

Paglen writes “Your health insurance will be modulated by the baby pictures your parents uploaded of you without your consent.” The company I work for, a smart home security company, collects data on how customers use their homes. We gather data on when, where and how crimes occur. This data can be anonymously sold to insurance providers to better understand the risks of their clients. This really seems like a fair way of conducting business, and perhaps a way to encourage people to act in a certain way to avoid risky behaviors. I believe where this may become dangerous is dangerous when we have profit-driven companies set agendas for how they believe people ought to behave. 

With every upload to Instagram or Snapchat we’re literally training algorithms to better interpret and comprehend human beings. AI networks are understanding places, objects, emotions, gestures, genders, economic statuses, relationships and more— often dictated by images created that are never viewed by humans, and I think that’s okay. 
